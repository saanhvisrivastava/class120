# -*- coding: utf-8 -*-
"""class120.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZZ2z-PqTPC2JAIn3biNMXNnKwH02rE2C
"""

import csv
import pandas as pd

from google.colab import files
data_to_load=files.upload()

import csv
import pandas as pd


df=pd.read_csv('class120.csv')

print(df.describe())

from sklearn.model_selection import train_test_split



X=df[['age','education-num','capital-gain','capital-loss','hours-per-week']]
Y=df['income']

X_train1,X_test1,Y_train1,Y_test1=train_test_split(X,Y,test_size=0.25,random_state=42)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

X_train1=sc.fit_transform(X_train1)
X_test1=sc.fit_transform(X_test1)

model1=GaussianNB()
model1.fit(X_train1,Y_train1)

y_pred1=model1.predict(X_test1)
accuracy=accuracy_score(Y_test1,y_pred1)
print(accuracy)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

X_train1=sc.fit_transform(X_train1)
X_test1=sc.fit_transform(X_test1)

model2=LogisticRegression(random_state=0)
model2.fit(X_train1,Y_train1)

y_pred2=model2.predict(X_test1)
accuracy=accuracy_score(Y_test1,y_pred2)
print(accuracy)

"""In the first data set of class both glucose and Bp had no correlation and both of them were contributing individually to predict whether the person has diabetes or not.

The dataset where all the features contribute individually to get the outcome,in this Naive bayes out performs Logistic Regression and is highly efficient.

In the 2nd dataset Logistic Regression out performed Naive bayes.REASON-Not all the features contributed individually to get the outcome,they were dependant on each other.
"""